class AlteredMNIST:
    """
    dataset description:
        X_I_L.png
        X: {aug=[augmented], clean=[clean]}
        I: {Index range(0, 60000)}
        L: {Labels range(10)}

    Write code to load Dataset
    """
    pass


class Encoder:
    """
    Write code for Encoder (Logits/embeddings shape must be [batch_size, channels, height, width])
    """
    pass


class Decoder:
    """
    Write code for Decoder here (Output image shape must be same as Input image shape i.e. [batch_size, 1, 28, 28])
    """
    pass


class AELossFn:
    """
    Loss function for AutoEncoder Training Paradigm
    """
    pass


class VAELossFn:
    """
    Loss function for Variational AutoEncoder Training Paradigm
    """
    pass


def ParameterSelector(E, D):
    """
    Write code for selecting parameters to train
    """
    pass


class AETrainer:
    """
    Write code for training AutoEncoder here.

    For each 10th minibatch use only this print statement
    print(">>>>> Epoch:{}, Minibatch:{}, Loss:{}, Similarity:{}".format(epoch, minibatch, loss, similarity))

    For each epoch use only this print statement
    print("----- Epoch:{}, Loss:{}, Similarity:{}")

    After every 5 epochs make 3D TSNE plot of logits of whole data and save the image as AE_epoch_{}.png
    """
    pass


class VAETrainer:
    """
    Write code for training Variational AutoEncoder here.

    For each 10th minibatch use only this print statement
    print(">>>>> Epoch:{}, Minibatch:{}, Loss:{}, Similarity:{}".format(epoch, minibatch, loss, similarity))

    For each epoch use only this print statement
    print("----- Epoch:{}, Loss:{}, Similarity:{}")

    After every 5 epochs make 3D TSNE plot of logits of whole data and save the image as VAE_epoch_{}.png
    """
    pass


class AE_TRAINED:
    """
    Write code for loading trained Encoder-Decoder from saved checkpoints for Autoencoder paradigm here.
    use forward pass of both encoder-decoder to get output image.
    """

    def from_path(sample, original, type):
        "Compute similarity score of both 'sample' and 'original' and return in float"
        pass


class VAE_TRAINED:
    """
    Write code for loading trained Encoder-Decoder from saved checkpoints for Autoencoder paradigm here.
    use forward pass of both encoder-decoder to get output image.
    """

    def from_path(sample, original, type):
        "Compute similarity score of both 'sample' and 'original' and return in float"
        pass


class CVAELossFn:
    """
    Write code for loss function for training Conditional Variational AutoEncoder
    """
    pass


class CVAE_Trainer:
    """
    Write code for training Conditional Variational AutoEncoder here.

    For each 10th minibatch use only this print statement
    print(">>>>> Epoch:{}, Minibatch:{}, Loss:{}, Similarity:{}".format(epoch, minibatch, loss, similarity))

    For each epoch use only this print statement
    print("----- Epoch:{}, Loss:{}, Similarity:{}")

    After every 5 epochs make 3D TSNE plot of logits of whole data and save the image as CVAE_epoch_{}.png
    """
    pass


class CVAE_Generator:
    """
    Write code for loading trained Encoder-Decoder from saved checkpoints for Conditional Variational Autoencoder paradigm here.
    use forward pass of both encoder-decoder to get output image conditioned to the class.
    """

    def save_image(digit, save_path):
        pass


def peak_signal_to_noise_ratio(img1, img2):
    if img1.shape[0] != 1:
        raise Exception("Image of shape [1, H, W] required.")

    img1, img2 = img1.to(torch.float64), img2.to(torch.float64)
    mse = img1.sub(img2).pow(2).mean()
    if mse == 0:
        return float("inf")
    else:
        return 20 * torch.log10(255.0/torch.sqrt(mse)).item()


def structure_similarity_index(img1, img2):
    if img1.shape[0] != 1:
        raise Exception("Image of shape [1, H, W] required.")

    window_size, channels = 11, 1
    K1, K2, DR = 0.01, 0.03, 255
    C1, C2 = (K1*DR)**2, (K2*DR)**2

    window = torch.randn(11)
    window = window.div(window.sum())
    window = window.unsqueeze(1).mul(window.unsqueeze(0)).unsqueeze(0).unsqueeze(0)

    mu1 = F.conv2d(img1, window, padding=window_size//2, groups=channels)
    mu2 = F.conv2d(img2, window, padding=window_size//2, groups=channels)
    mu12 = mu1.pow(2).mul(mu2.pow(2))

    sigma1_sq = F.conv2d(img1 * img1, window, padding=window_size//2, groups=channels) - mu1.pow(2)
    sigma2_sq = F.conv2d(img2 * img2, window, padding=window_size//2, groups=channels) - mu2.pow(2)
    sigma12 =  F.conv2d(img1 * img2, window, padding=window_size//2, groups=channels) - mu12

    SSIM_n = (2 * mu1 * mu2 + C1) * (2 * sigma12 + C2)
    denom = ((mu1**2 + mu2**2 + C1) * (sigma1_sq + sigma2_sq + C2))
    return torch.clamp((1 - SSIM_n / (denom + 1e-8)), min=0.0, max=1.0).mean().item()