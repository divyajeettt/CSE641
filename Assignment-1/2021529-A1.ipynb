{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **CSE641: *Deep Learing Assignment-1***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset, Subset, random_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install idx2numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "import time\n",
    "import random\n",
    "import idx2numpy\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "from PIL import Image\n",
    "from typing import Callable\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Question: 1**\n",
    "\n",
    "For Question-1, we implement the `pytorch.utils.data.Dataset` and `pytorch.utils.data.DataLoader` classes from scratch in Python. Specifically, we make use of the `MNIST` dataset, hence the `torchvision.datasets.MNIST`. Using the template provided, we implement the classes `MNISTDataset` and `MyDataLoader`. Finally, the performance of the scratch implementation is compared with the `torch` implementations by plotting a graph of batch-size vs total data loading time.\n",
    "\n",
    "### **References**\n",
    "\n",
    "1. [`torch.utils.data.DataLoader`, `torch.utils.data.Dataset`](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)\n",
    "2. [`torchvision.datasets.MNIST`](https://pytorch.org/vision/stable/_modules/torchvision/datasets/mnist.html#MNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTDataset:\n",
    "\tULRLs: tuple[str] = (\n",
    "\t\tr\"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\",\n",
    "\t\tr\"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\"\n",
    "\t\tr\"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\",\n",
    "\t\tr\"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\"\n",
    "\t)\n",
    "\n",
    "\tdef __init__(self, root: str, train: bool|None = False, download: bool|None = False, transform: Callable|None = None, target_transform: Callable|None = None):\n",
    "\t\tself.root = os.path.join(root, \"MNIST\")\n",
    "\t\tself.train = train\n",
    "\t\tif download and not os.path.exists(self.root):\n",
    "\t\t\tself.download()\n",
    "\t\tself.transform = transform\n",
    "\t\tself.target_transform = target_transform\n",
    "\t\tself._load()\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.data)\n",
    "\n",
    "\tdef __getitem__(self, idx: int) -> tuple[_, int]:\n",
    "\t\timage, target = Image.fromarray(self.data[idx]), int(self.targets[idx])\n",
    "\t\tif self.transform is not None:\n",
    "\t\t\timage = self.transform(image)\n",
    "\t\tif self.target_transform is not None:\n",
    "\t\t\ttarget = self.target_transform(target)\n",
    "\t\treturn image, target\n",
    "\n",
    "\tdef download(self) -> None:\n",
    "\t\tdirectory = os.path.join(self.root, \"raw\")\n",
    "\t\tos.makedirs(directory, exist_ok=True)\n",
    "\t\tfor url in MNISTDataset.URLs:\n",
    "\t\t\tfilename = os.path.join(directory, os.path.basename(url))\n",
    "\t\t\tdestination = filename.replace(\".gz\", \"\")\n",
    "\t\t\turllib.request.urlretrieve(url, filename)\n",
    "\t\t\twith gzip.open(filename, \"rb\") as f_in:\n",
    "\t\t\t\twith open(destination, \"wb\") as f_out:\n",
    "\t\t\t\t\tf_out.write(f_in.read())\n",
    "\n",
    "\tdef _load(self) -> None:\n",
    "\t\tprefix = \"train\" if self.train else \"t10k\"\n",
    "\t\timages_filename = os.path.join(self.root, \"raw\", f\"{prefix}-images-idx3-ubyte\")\n",
    "\t\tlabels_filename = os.path.join(self.root, \"raw\", f\"{prefix}-labels-idx1-ubyte\")\n",
    "\t\tself.data = idx2numpy.convert_from_file(images_filename)\n",
    "\t\tself.targets = torch.tensor(idx2numpy.convert_from_file(labels_filename), dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataLoader:\n",
    "    def __init__(self, dataset: MNISTDataset|Dataset, batch_size: int|None = 1, shuffle: bool|None = False):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.dataset_size = len(dataset)\n",
    "        self.indices = torch.arange(self.dataset_size)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return (self.dataset_size + self.batch_size - 1) // self.batch_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self.shuffle:\n",
    "            random.shuffle(self.indices)\n",
    "        self.current_index = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.current_index >= self.dataset_size:\n",
    "            raise StopIteration\n",
    "        batch_indices = self.indices[self.current_index:self.current_index+self.batch_size]\n",
    "        data_batch = torch.stack([self.dataset[i][0] for i in batch_indices])\n",
    "        target_batch = self.dataset.targets[batch_indices]\n",
    "        self.current_index += self.batch_size\n",
    "        return data_batch, target_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_training_data = torchvision.datasets.MNIST(root=\"data\", train=True, download=True, transform=transforms.ToTensor())\n",
    "custom_training_data = MNISTDataset(root=\"data\", train=True, download=True, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = torch.zeros(2, 8)\n",
    "REPEATS = 10\n",
    "\n",
    "times = torch.load(\"average_times_per_batch.pt\")\n",
    "print(\"Loaded average times\")\n",
    "print(times)\n",
    "\n",
    "# for i in range(8):\n",
    "#     batch_size = pow(2, i+5)\n",
    "#     torch_train_dataloader = DataLoader(torch_training_data, batch_size=batch_size, shuffle=False)\n",
    "#     custom_train_dataloader = CustomDataLoader(custom_training_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "#     for j in range(REPEATS):\n",
    "#         print(f\"Batch size: {batch_size}, repeat: {j}\", end=\"\\r\")\n",
    "\n",
    "#         start = time.time()\n",
    "#         for data, target in torch_train_dataloader: pass\n",
    "#         times[0, i] += ((time.time() - start) / len(torch_train_dataloader))\n",
    "\n",
    "#         start = time.time()\n",
    "#         for data, target in custom_train_dataloader: pass\n",
    "#         times[1, i] += ((time.time() - start) / len(custom_train_dataloader))\n",
    "\n",
    "# print(times := times/(REPEATS*8))\n",
    "# torch.save(times, \"average_times_per_batch.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([2**i for i in range(5, 13)], times[0], label=\"PyTorch\", marker=\"o\")\n",
    "plt.plot([2**i for i in range(5, 13)], times[1], label=\"Scratch-Implementation\", marker=\"o\")\n",
    "\n",
    "plt.xlabel(\"Batch size\")\n",
    "plt.ylabel(\"Average Time taken to load one batch (s)\")\n",
    "plt.legend()\n",
    "\n",
    "plt.xscale(\"log\", base=2)\n",
    "plt.ylim(0, 0.05)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Question: 2**\n",
    "\n",
    "In this question, we implement a simple Feed-Forward Neural Netowrk using Pytorch with the following specifications:\n",
    "- 4 Hidden Layers\n",
    "- At least 32 Nodes in each Hidden Layer\n",
    "- Load the data using the more effective dataloader as identified in Question-1\n",
    "- Activation Function: [`torch.nn.ReLU`](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html)\n",
    "- Loss Function: [`torch.nn.CrossEntropyLoss`](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)\n",
    "- Optimizer: [`torch.optim.SGD` (Stochastic Gradient Descent)](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html)\n",
    "- Learning Rate: 0.0003\n",
    "\n",
    "Finally, we plot graphs of the loss and accuracy vs number of epochs for training, testing, and validation data for 60 epochs.\n",
    "\n",
    "### **References**\n",
    "\n",
    "1. [PyTorch Crash Course - Getting Started with Deep Learning, YouTube](https://www.youtube.com/watch?v=OIenNRt2bjg)\n",
    "2. [PyTorch Official Documentation](https://pytorch.org/docs/stable/index.html)\n",
    "\n",
    "### **Observations**\n",
    "\n",
    "1. Even with 4 hidden layers of 128 neurons each, the model achieves a menial accuracy of 12%.\n",
    "2. The learning rate 0.0003 seems to be too small for the model, and the model often gets stuck in some local minima - achieving no improvement in loss or accuracy via updates.\n",
    "3. With a higher learning rate of 0.003, the loss of the model decreases significantly, but the accuracy remains in the same range. This is probably due to overfitting - we can reduce the number of hidden layers and number of epochs for a better generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Common Hyperparameters and Data for all models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE: int = 28 * 28\n",
    "HIDDEN_SIZE: list[int] = [45, 35, 35, 32]\n",
    "OUTPUT_SIZE: int = 10\n",
    "EPOCHS: int = 60\n",
    "LR: float = 3e-4\n",
    "BATCH_SIZE: int = 75\n",
    "\n",
    "train_data = MNISTDataset(root=\"data\", train=True, download=True, transform=transforms.ToTensor())\n",
    "test_data = MNISTDataset(root=\"data\", train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "size = int(0.8 * len(train_data))\n",
    "train_data, val_data = random_split(train_data, [size, len(train_data) - size])\n",
    "\n",
    "TRAIN_LOADER = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "TEST_LOADER = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "VAL_LOADER = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    ACCURACIES: np.ndarray\n",
    "    LOSSES: np.ndarray\n",
    "\n",
    "    def __init__(self, input_size: int, hidden_size: list[int], output_size: int, activation: nn.Module):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(input_size, hidden_size[0]))\n",
    "        layers.append(activation())\n",
    "        # layers.append(nn.BatchNorm1d(hidden_size[0], affine=False))\n",
    "        for i in range(len(hidden_size)-1):\n",
    "            layers.append(nn.Linear(hidden_size[i], hidden_size[i+1]))\n",
    "            layers.append(activation())\n",
    "            # layers.append(nn.BatchNorm1d(hidden_size[i+1], affine=False))\n",
    "        layers.append(nn.Linear(hidden_size[-1], output_size))\n",
    "\n",
    "        self.LOSSES = np.zeros((3, 1))\n",
    "        self.ACCURACIES = np.zeros((3, 1))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.layers(x)\n",
    "\n",
    "    def train(self, dataloader: list[CustomDataLoader|DataLoader], epochs: int, lr: float, verbose: bool|None = False) -> None:\n",
    "        self.ACCURACIES.resize((3, epochs))\n",
    "        self.LOSSES.resize((3, epochs))\n",
    "\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "        for epoch in range(epochs):\n",
    "            for batch, (images, labels) in enumerate(dataloader[0], start=1):\n",
    "                images = (images - images.mean()) / images.std()\n",
    "                outputs = self.forward(images.reshape(-1, 28*28))\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for index, loader in enumerate(dataloader):\n",
    "                    self.evaluate(loader, index, epoch)\n",
    "\n",
    "            if verbose:\n",
    "                epoch_ = f\"Epoch [{str(epoch+1).zfill(2)}/{epochs}]\"\n",
    "                accuracy_, loss_ = self.ACCURACIES[0, epoch], self.LOSSES[0, epoch]\n",
    "                print(f\"{epoch_}, (Training) Accuracy: {accuracy_:.10f}, (Training) Loss: {loss_:.10f}\", end=\"\\r\")\n",
    "\n",
    "    def evaluate(self, dataloader: CustomDataLoader|DataLoader, index: int, epoch: int) -> None:\n",
    "        loss = correct = total = 0\n",
    "        for images, labels in dataloader:\n",
    "            total += labels.shape[0]\n",
    "            outputs = self.forward(images.reshape(-1, 28*28))\n",
    "            loss += self.criterion(outputs, labels).item()\n",
    "            correct += (outputs.argmax(axis=1) == labels).sum().item()\n",
    "\n",
    "        self.ACCURACIES[index, epoch] = correct / total\n",
    "        self.LOSSES[index, epoch] = loss / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(INPUT_SIZE, HIDDEN_SIZE, OUTPUT_SIZE, activation=nn.ReLU)\n",
    "model.train([TRAIN_LOADER, TEST_LOADER, VAL_LOADER], EPOCHS, LR, verbose=True)\n",
    "# torch.save(model.state_dict(), \"model_torch_rulu.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, EPOCHS+1), model.LOSSES[0], label=\"Train\", marker=\".\")\n",
    "plt.plot(range(1, EPOCHS+1), model.LOSSES[1], label=\"Test\", marker=\".\")\n",
    "plt.plot(range(1, EPOCHS+1), model.LOSSES[2], label=\"Validation\", marker=\".\")\n",
    "\n",
    "plt.title(\"Loss vs Epochs\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, EPOCHS+1), model.ACCURACIES[0], label=\"Train\", marker=\".\")\n",
    "plt.plot(range(1, EPOCHS+1), model.ACCURACIES[1], label=\"Test\", marker=\".\")\n",
    "plt.plot(range(1, EPOCHS+1), model.ACCURACIES[2], label=\"Validation\", marker=\".\")\n",
    "\n",
    "plt.title(\"Accuracy vs Epochs\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "\n",
    "plt.ylim(-0.1, 1.1)\n",
    "plt.yticks(np.arange(0, 1.1, 0.1))\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Question: 3**\n",
    "\n",
    "In this question, we are required to write the FeedForward network from scratch using only PyTorch Tensor operations, while manually implementing the trianing loop and gradient calculations. The same hyperparameters as in Question-2 are used. The model is trained for 60 epochs, and the loss and accuracy vs number of epochs for training, testing, and validation data are plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"\n",
    "    Abstract class to represent a Layer of a Neural Network\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self, grad: torch.Tensor) -> torch.Tensor:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def update(self, lr: float) -> None:\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class Linear(Layer):\n",
    "    def __init__(self, input_size: int, output_size: int):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.weights = torch.rand(input_size, output_size) * np.sqrt(2 / input_size)\n",
    "        self.bias = torch.rand(output_size) * np.sqrt(2 / input_size)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        with torch.no_grad():\n",
    "            self.x = x\n",
    "            return self.x @ self.weights + self.bias\n",
    "\n",
    "    def backward(self, grad: torch.Tensor) -> torch.Tensor:\n",
    "        with torch.no_grad():\n",
    "            self.weights_grad = self.x.T @ grad\n",
    "            self.bias_grad = grad.sum(axis=0)\n",
    "            return grad @ self.weights.T\n",
    "\n",
    "    def update(self, lr: float) -> None:\n",
    "        with torch.no_grad():\n",
    "            self.weights -= lr * self.weights_grad\n",
    "            self.bias -= lr * self.bias_grad\n",
    "\n",
    "\n",
    "class ReLU(Layer):\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        with torch.no_grad():\n",
    "            self.x = x\n",
    "            return torch.max(x, torch.zeros_like(x))\n",
    "\n",
    "    def backward(self, grad: torch.Tensor) -> torch.Tensor:\n",
    "        with torch.no_grad():\n",
    "            return grad * (self.x > 0).float()\n",
    "\n",
    "    def update(self, lr: float) -> None:\n",
    "        pass\n",
    "\n",
    "\n",
    "class Sigmoid(Layer):\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        with torch.no_grad():\n",
    "            self.sigmoid = 1 / (1 + torch.exp(-x))\n",
    "            return self.sigmoid\n",
    "\n",
    "    def backward(self, grad: torch.Tensor) -> torch.Tensor:\n",
    "        with torch.no_grad():\n",
    "            return grad * self.sigmoid * (1 - self.sigmoid)\n",
    "\n",
    "    def update(self, lr: float) -> None:\n",
    "        pass\n",
    "\n",
    "\n",
    "# class BatchNorm(Layer):\n",
    "#     def __init__(self, input_size: int):\n",
    "#         self.input_size = input_size\n",
    "\n",
    "#     def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "#         with torch.no_grad():\n",
    "#             self.x = x\n",
    "#             self.mean = x.mean(axis=0)\n",
    "#             self.std = torch.clamp(x.std(axis=0), 1e-12, 1e12)\n",
    "#             return (self.x - self.mean) / self.std\n",
    "\n",
    "#     def backward(self, grad: torch.Tensor) -> torch.Tensor:\n",
    "#         with torch.no_grad():\n",
    "#             return grad / self.std\n",
    "\n",
    "#     def update(self, lr: float) -> None:\n",
    "#         pass\n",
    "\n",
    "\n",
    "class CrossEntropyLoss:\n",
    "    def __call__(self, outputs: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "        with torch.no_grad():\n",
    "            outputs = torch.clamp(outputs, 1e-12, 1 - 1e-12)\n",
    "            self.outputs = outputs.softmax(axis=1)\n",
    "            self.targets = targets\n",
    "            return -torch.log(self.outputs[range(len(targets)), targets]).mean()\n",
    "\n",
    "    def backward(self) -> torch.Tensor:\n",
    "        with torch.no_grad():\n",
    "            return (self.outputs - torch.eye(self.outputs.shape[1])[self.targets]) / len(self.targets)\n",
    "\n",
    "\n",
    "class SGD:\n",
    "    def __init__(self, parameters: list[Layer], lr: float):\n",
    "        self.parameters = parameters[::-1]\n",
    "        self.lr = lr\n",
    "\n",
    "    def step(self, grad: torch.Tensor) -> None:\n",
    "        with torch.no_grad():\n",
    "            for parameter in self.parameters:\n",
    "                grad = parameter.backward(grad)\n",
    "                parameter.update(self.lr)\n",
    "\n",
    "\n",
    "class CustomNeuralNetwork:\n",
    "    ACCURACIES: np.ndarray\n",
    "    LOSSES: np.ndarray\n",
    "\n",
    "    def __init__(self, input_size: int, hidden_size: list[int], output_size: int, activation: Layer):\n",
    "        self.layers = []\n",
    "        self.layers.append(Linear(input_size, hidden_size[0]))\n",
    "        self.layers.append(activation())\n",
    "        # self.layers.append(BatchNorm(hidden_size[0]))\n",
    "        for i in range(len(hidden_size)-1):\n",
    "            self.layers.append(Linear(hidden_size[i], hidden_size[i+1]))\n",
    "            self.layers.append(activation())\n",
    "            # self.layers.append(BatchNorm(hidden_size[i+1]))\n",
    "        self.layers.append(Linear(hidden_size[-1], output_size))\n",
    "\n",
    "        self.LOSSES = np.zeros((3, 1))\n",
    "        self.ACCURACIES = np.zeros((3, 1))\n",
    "        self.criterion = CrossEntropyLoss()\n",
    "\n",
    "    def parameters(self) -> list[Layer]:\n",
    "        return self.layers\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def train(self, dataloader: list[CustomDataLoader|DataLoader], epochs: int, lr: float, verbose: bool|None = False) -> None:\n",
    "        self.ACCURACIES.resize((3, epochs))\n",
    "        self.LOSSES.resize((3, epochs))\n",
    "\n",
    "        optimizer = SGD(model.parameters(), lr=lr)\n",
    "        for epoch in range(epochs):\n",
    "            for batch, (images, labels) in enumerate(dataloader[0], start=1):\n",
    "                outputs = self.forward(images.reshape(-1, 28*28))\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                optimizer.step(self.criterion.backward())\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for index, loader in enumerate(dataloader):\n",
    "                    self.evaluate(loader, index, epoch)\n",
    "\n",
    "            if verbose:\n",
    "                epoch_ = f\"Epoch [{str(epoch+1).zfill(2)}/{epochs}]\"\n",
    "                accuracy_, loss_ = self.ACCURACIES[0, epoch], self.LOSSES[0, epoch]\n",
    "                print(f\"{epoch_}, (Training) Accuracy: {accuracy_:.10f}, (Training) Loss: {loss_:.10f}\", end=\"\\r\")\n",
    "\n",
    "    def evaluate(self, dataloader: CustomDataLoader|DataLoader, index: int, epoch: int) -> None:\n",
    "        loss = correct = total = 0\n",
    "        for images, labels in dataloader:\n",
    "            total += labels.shape[0]\n",
    "            outputs = self.forward(images.reshape(-1, 28*28))\n",
    "            loss += self.criterion(outputs, labels).item()\n",
    "            correct += (outputs.argmax(axis=1) == labels).sum().item()\n",
    "\n",
    "        self.ACCURACIES[index, epoch] = correct / total\n",
    "        self.LOSSES[index, epoch] = loss / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomNeuralNetwork(INPUT_SIZE, HIDDEN_SIZE, OUTPUT_SIZE, activation=ReLU)\n",
    "model.train([TRAIN_LOADER, TEST_LOADER, VAL_LOADER], EPOCHS, LR, verbose=True)\n",
    "# torch.save(model.state_dict(), \"model_scratch_rulu.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Question: 4**\n",
    "\n",
    "In this question, we repreat both Question 2 and 3, but just using Sigmoid activation function instead of ReLU. The same hyperparameters as in Question-2 are used. The model is trained for 60 epochs, and the loss and accuracy vs number of epochs for training, testing, and validation data are plotted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **PyTorch Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(INPUT_SIZE, HIDDEN_SIZE, OUTPUT_SIZE, activation=nn.Sigmoid)\n",
    "model.train([TRAIN_LOADER, TEST_LOADER, VAL_LOADER], EPOCHS, LR, verbose=True)\n",
    "# torch.save(model.state_dict(), \"model_torch_sigmoid.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Scratch Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomNeuralNetwork(INPUT_SIZE, HIDDEN_SIZE, OUTPUT_SIZE, activation=Sigmoid)\n",
    "model.train([TRAIN_LOADER, TEST_LOADER, VAL_LOADER], EPOCHS, LR, verbose=True)\n",
    "# torch.save(model.state_dict(), \"model_scratch_sigmoid.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
